{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc75a986-11d2-4bde-bda9-44f1b408936b",
   "metadata": {},
   "source": [
    "# Element 1 Test - Image Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "836a73ac-5ad1-4080-9600-6d376d14eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274514fb-f474-42b2-abc5-598bb909d021",
   "metadata": {},
   "source": [
    "## Image Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "268c7254-4cf1-48a6-a7b2-edb4528b6061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed name of 0 images\n",
      "Deleted 0 images\n"
     ]
    }
   ],
   "source": [
    "num_skipped = 0\n",
    "num_changed = 0\n",
    "for root, dirs, files in os.walk(\"./data\"):\n",
    "  path = root.split(os.sep)\n",
    "\n",
    "  for file in enumerate(files):\n",
    "    _, extension = os.path.splitext(file[1])\n",
    "    if extension == \".jpg\" or extension == \".jpeg\" or extension == \".png\":\n",
    "      filepath = root + \"/\" + file[1]\n",
    "        \n",
    "      if not file[1].strip(extension).isascii():\n",
    "          num_changed += 1\n",
    "          new_file_name = \"image\" + str(file[0]) + extension\n",
    "          new_filepath = root + \"/\" + new_file_name\n",
    "          os.rename(filepath, new_filepath)\n",
    "          filepath = new_filepath\n",
    "    \n",
    "      try:\n",
    "        fobj = open(filepath, \"rb\")\n",
    "        is_good = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10) or tf.compat.as_bytes(\"PNG\") in fobj.peek(10)\n",
    "      finally:\n",
    "        fobj.close()\n",
    "\n",
    "      if not is_good:\n",
    "        num_skipped += 1\n",
    "        # Delete corrupted image\n",
    "        os.remove(filepath)\n",
    "\n",
    "print(\"Changed name of %d images\" % num_changed)\n",
    "print(\"Deleted %d images\" % num_skipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7913226b-3308-4aca-8375-bd43185bc5ef",
   "metadata": {},
   "source": [
    "This is taken from week 10 Data Processing with some changes. It walks through every image in our dataset and removes any file that does not fit the perameters. Here, this means that jpg, jpeg and png images that don't have a JFIF or PNG header will be removed. Keras also supports bmp and gif but there doesn't seem like much of a point to look for these.\n",
    "Some file names where also causing an error while tring to load data because their file name were not utf-8, so I added a few lines that checks the file names are ascii and renames them if not.\n",
    "\n",
    "The first run removed **249** images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a93bf0-e6c6-481b-aeeb-800411a26894",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "621cd7c4-66e4-4553-8450-b14b0afb06ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1273 files belonging to 4 classes.\n",
      "Using 1146 files for training.\n",
      "Found 1273 files belonging to 4 classes.\n",
      "Using 127 files for validation.\n"
     ]
    }
   ],
   "source": [
    "images_path = \"./data\"\n",
    "batch_size=16\n",
    "shape=(300,300,3)\n",
    "\n",
    "training_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    images_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=shape[:2],\n",
    "    subset='training',\n",
    "    validation_split=0.1,\n",
    "    seed=42,\n",
    "    pad_to_aspect_ratio=True\n",
    ")\n",
    "\n",
    "test_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    images_path,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=shape[:2],\n",
    "    subset='validation',\n",
    "    validation_split=0.1,\n",
    "    seed=42,\n",
    "    pad_to_aspect_ratio=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b0c84-d5b0-40fe-a0cd-3969c51451a4",
   "metadata": {},
   "source": [
    "For this model there are 4 categories: bicycle, car, deer and mountain. This loads our data from the dataset and splits it into two more datasets, one for training and one for testing. Both also have a small portion set aside for validation.\n",
    "- I've made sure to set the label to to catargorical, as the task is to catagorise the images.\n",
    "- The batch size offers a reasonable training time and good accuracy. Making it a little smaller than default (32) could help with getting better validation accuracy and adding some noise to the model. Increasing it would improve training times.\n",
    "- I have made the image size 300 by 300 as this helps to reduce training time whislt still keeping important features of the images. I could possibly increase this for a boost in accuracy with a sacrifice of speed, or vice versa. Note that many of the photos (like panoramas) would get quite squished which might also affect performance. I've added pad_to_aspect_ratio to help with this as we're looking for objects in an entire image and cropping them to fit the frame would not be wise.\n",
    "- The validation split is set so that in training I can see how well the modal in generalizing, or if it is strating to overfit\n",
    "- There is also a default colour mode of rgb, meaning that any black and white images will be coverted to 3 channels to fit the shape of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c44118-f344-40bd-9226-f3edae5242be",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "379b73f7-d9b3-41c9-a5f5-7df8b648f2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_29\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_29\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ random_flip_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomFlip</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ random_rotation_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomRotation</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ random_brightness_30                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomBrightness</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ random_contrast_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RandomContrast</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_181 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">298</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_142 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">149</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">149</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_182 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">147</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">147</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,168</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_143 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">73</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_183 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">71</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_144 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout2d_28                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_184 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ average_pooling2d_7                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_185 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_145 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout2d_29                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout2D</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_186 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_29          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_116 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_117 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_118 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_119 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ random_flip_30 (\u001b[38;5;33mRandomFlip\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ random_rotation_30 (\u001b[38;5;33mRandomRotation\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ random_brightness_30                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mRandomBrightness\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ random_contrast_30 (\u001b[38;5;33mRandomContrast\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_181 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m298\u001b[0m, \u001b[38;5;34m298\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m224\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_142 (\u001b[38;5;33mMaxPooling2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m149\u001b[0m, \u001b[38;5;34m149\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_182 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m147\u001b[0m, \u001b[38;5;34m147\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m1,168\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_143 (\u001b[38;5;33mMaxPooling2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m73\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_183 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m71\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m4,640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_144 (\u001b[38;5;33mMaxPooling2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout2d_28                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_184 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m33\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ average_pooling2d_7                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_185 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_145 (\u001b[38;5;33mMaxPooling2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout2d_29                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout2D\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_186 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m295,168\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d_29          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_29 (\u001b[38;5;33mFlatten\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_116 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m16,448\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_117 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m8,320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_118 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_29 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_119 (\u001b[38;5;33mDense\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │             \u001b[38;5;34m260\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">426,836</span> (1.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m426,836\u001b[0m (1.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">426,836</span> (1.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m426,836\u001b[0m (1.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.Input(shape=shape),\n",
    "    keras.layers.RandomFlip(mode=\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomBrightness(0.2),\n",
    "    keras.layers.RandomContrast(0.2),\n",
    "    \n",
    "    keras.layers.Conv2D(8, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    keras.layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.SpatialDropout2D(0.1),\n",
    "    \n",
    "    keras.layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\"),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d72faf-a84e-4f92-a5d8-ce0c031cf638",
   "metadata": {},
   "source": [
    "After some extensive testing and looking at other smaller models such as MobileNetV2 I've found that these layers work out to have reasonable accuracy. The convolutional layers seem to get better results with ReLU instead of sigmoid, which looks to be fairly standard for other CNNs.\n",
    "Keras documentation also recommends the use of softmax for catagorical tasks with more than two catagories, otherwise I would use sigmoid. The difference between sigmoid and softmax here is quite small. There is also a spacial dropout and dropout layers to help prevent overfitting. Finally, I have a mix of pooling layers as each help with finding different aspects of images. Max is most prominent because in most of the images we are looking for details withing the image like deer, bikes or cars. Global average pooling helps looking at the overall image, helping to look for images with mountains. Whilst these might not make a huge difference they might still help accuracy a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2e180304-1705-4013-baa5-48b107ce94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d22cd2-b882-4926-a19e-7030191f1618",
   "metadata": {},
   "source": [
    "Loss is set to categorical crossentropy, as we are categorising images into four categories. I've also set the metrics to categorical accuracy for the same reason. I have also left the learning rate as default (0.001) as it seems to be working well and is adaptive with the adam optimizer. Other opitmizers like SGD might work too, but would require a lot more fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e40357cf-a58d-496f-8ae3-6d2b0c06fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 463ms/step - categorical_accuracy: 0.7811 - loss: 0.5812 - val_categorical_accuracy: 0.7795 - val_loss: 0.5724\n",
      "Epoch 2/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 465ms/step - categorical_accuracy: 0.7640 - loss: 0.5466 - val_categorical_accuracy: 0.8031 - val_loss: 0.5433\n",
      "Epoch 3/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 482ms/step - categorical_accuracy: 0.7836 - loss: 0.5456 - val_categorical_accuracy: 0.8031 - val_loss: 0.5095\n",
      "Epoch 4/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 466ms/step - categorical_accuracy: 0.8001 - loss: 0.5099 - val_categorical_accuracy: 0.7559 - val_loss: 0.7007\n",
      "Epoch 5/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 467ms/step - categorical_accuracy: 0.7767 - loss: 0.5408 - val_categorical_accuracy: 0.7559 - val_loss: 0.6355\n",
      "Epoch 6/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 468ms/step - categorical_accuracy: 0.8026 - loss: 0.5242 - val_categorical_accuracy: 0.7559 - val_loss: 0.6406\n",
      "Epoch 7/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 469ms/step - categorical_accuracy: 0.8180 - loss: 0.5401 - val_categorical_accuracy: 0.8110 - val_loss: 0.5870\n",
      "Epoch 8/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 469ms/step - categorical_accuracy: 0.8031 - loss: 0.4925 - val_categorical_accuracy: 0.7638 - val_loss: 0.6309\n",
      "Epoch 9/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 468ms/step - categorical_accuracy: 0.8087 - loss: 0.4779 - val_categorical_accuracy: 0.7638 - val_loss: 0.6338\n",
      "Epoch 10/10\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 484ms/step - categorical_accuracy: 0.8108 - loss: 0.4960 - val_categorical_accuracy: 0.7795 - val_loss: 0.7045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fb1a092590>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  training_dataset,\n",
    "  validation_data=test_dataset,\n",
    "  epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f737bfa7-6051-4463-b148-c70850b9de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 267ms/step - categorical_accuracy: 0.8072 - loss: 0.6281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7045027017593384, 0.7795275449752808]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e38541-117a-40aa-92f0-24dba913c075",
   "metadata": {},
   "source": [
    "After 20 epochs the highest test accuracy I have been able to achieve is around 72%. However, only a few epochs in, the models loss continues to decrease but validation loss stagnates - this could be a sign of overfitting or it could be because I am using a smaller batch size, making each step less accurate meaning I need more epochs. This requires more testing. <br>\n",
    "To try and fix this I've added some extra data [processing](https://keras.io/api/layers/preprocessing_layers/image_augmentation/) to increase the size of the dataset.\n",
    "I've also added random contrast and random brightness to the model to further increase the amount of data. This seems to have imporved validation loss as well as the testing accuracy but only by 1-2%. <br>\n",
    "<br>\n",
    "I could further reduce batch sizing; increase the image size and add more convolutional layers; and look into normalization and regularization; or gather more data to get a better model. But all of these increase training time and need for computational power. One other thing I could do it filter the image dataset a bit more. Most of the images are valid however some images don't correspont with their labels at all - I would probably have to do this manually.\n",
    "<br>\n",
    "\n",
    "edit: I added some dense layers at the end of the model for fun at it looks to have improved the consistancy of testing accuracy. This is also where I added pad to aspect ration, which all together gives testing results between 73%-80% after some longer training (50 epochs). However, this is with 127 images for testing, which might not be enough to give a good metric on accuracy. A bigger dataset would help here, and also in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b6efb-f396-4663-99d9-caf9df260b79",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "fe653cbf-f1f5-4eb5-a50a-6e37a1564a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./Element1_CNN.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fdd1730a-ade6-4bb7-8152-4cf4e6d7e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"./Element1_CNN.keras\")\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11147a53-b180-43dd-a15d-2573b7eca556",
   "metadata": {},
   "source": [
    "Finally, the model has been saved. To use it go to ImageClassifier.ipynb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
